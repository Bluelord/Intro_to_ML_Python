{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjHZR7x5+RwAwJG418sDE3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bluelord/Intro_to_ML_Python_Notes/blob/master/Supervised_Learning/Linear_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qegWlC-QNZG"
      },
      "source": [
        "### **Linear Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qxunl4dQfnz"
      },
      "source": [
        "#### **Linear model for regression**\n",
        "\n",
        "Linear models make a prediction using a *linear function* of the input features, for regression the general prediction formula is given as\n",
        "\n",
        "$ \\hat{y} = w[0]*x[0] +  w[1]*x[1] + ... + w[p]*x[p] + b$\n",
        "\n",
        "$x[0]$ to $x[p]$ are the **p** features of a single data points, $w$ & $b$ are parametes of the model that are learned and $\\hat{y}$ is the prediction. $w$ is slope along each features axis and $b$ is offset value. The predicted value is a weighted sum of the input features (weight as $w$).\n",
        "\n",
        "Linear Model can be chrecterized as regression model for which the predictions are in a line for single feature, a plane for two feature or a hyperplane fir higher dimentions. When we have many features, linear model can be powerful tool to predict the output, aspecially when there is more features than data points.\n",
        "\n",
        "There are different linear models for regression depanding upon how model parameters $w$ & $b$ are learned and how the model complexity can be controlled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksvnJQD-QPVq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JT5-E9bQ1sy"
      },
      "source": [
        "**Linear Regression (aka ordinary least squares)**\n",
        "\n",
        "In this the parameters $w$ & $b$ are found by minimizing the *mean sqare error* between prediction and the true regression target $y$ on training set. Linear Regression has no parameter to control the model complexity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2m7ToHRQ2YS"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO5FGck5Q_ST"
      },
      "source": [
        "At low feature number model is most likly to be underfitting,when number of features are very high there is a higher chance of overfitting. Thus when we find such overfitting case we have to fine a model in which wr can control the model complexity, such alternatives are Ridge Regression and LASSO."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MOBxx7BQ_4R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BINyIQ0fRCfE"
      },
      "source": [
        "**Ridge Regression**\n",
        "\n",
        "In this type of linear model, complexity controlling parameter is added to the ordinary linear regression model. By adding this parameter, most of the coeficient $w$ should become close to zero, which mean the effect of features on the output will be as little ad possible, this effect is called ad regularization. This regularization avoids the model to get overfitted, this regression regularization is also called as *L2 regularization*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6t-pgXaRGFp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEqt0sTARJhz"
      },
      "source": [
        "Ridge is more restricted model, so its less likly to overfit. Less complex model means worse performance on trainig but better generalization. Ridge model can make trade-off between the simplicity of model and its performance on training set by choosing the *alpha parameter*. This value depends on the dataset, inceasing *alpha* forces coefficients to move more towards zero and decreasing it will make model less restricted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCeTnmLxRLjh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxu3UCKiRMAB"
      },
      "source": [
        "LASSO\n",
        "\n",
        "LASSO regression also restrict the coefficients, it is also called as *L1 regularization*. While using Lasso regression we are expected taht some of the coefficients hets exactly zero, which means this features are entirly ignored by the model. We can also consider LASSO as a form of automatic feature selection model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw42T5FURWp1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX4_msOCRXRT"
      },
      "source": [
        "In practice, Ridge is the first choice and if we are expect only few feature from the large feature dataset then LASSO is the best choice. scikit has **ElasticNet class* which combines the penalties (regularization term) of Lasso and Ridge, which works best at the price of tuning two parameters, L1 & L2 regularuzations to adjest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnuoIhXLRdyG"
      },
      "source": [
        "#### **Linear models for classification**\n",
        "\n",
        "The formula for classification is similar to that of regression, but instead of returning weighted sum we threshold the value at zero. If the value less than 0 be predict it as class -1 or else we predict the class as +1. In classification, the *decision boundary* is a linear function of input, which can be a line for twtwo classes or hyperplan for multiclas.\n",
        "\n",
        "The algorithms differ in two ways, how well $w$ & $b$ fits the training data and what kind of regularization is used. Two most common algorithms are *Logistic Regression* and *linear support vector machines* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bag0GLpvRaQB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG0gldtMR0Fq"
      },
      "source": [
        "The performance of classification is similar to that of regression in similar sinario like high dimentions large darapoitns etc, Both LR and SVC, by default uses L2 regularization. The parameter use for regularization is called **C**, this emphasis on finding a coefficient. Higher value of C can overfit the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL55P6s5R0xQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr5g4AjDR4UC"
      },
      "source": [
        "**Linear Model for multiclass classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz26VwaLR5Sg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}