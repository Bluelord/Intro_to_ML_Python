{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive_Bayes_Classifiers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO8tOLsqmE5vtoXYmC1nAfc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bluelord/Intro_to_ML_Python_Notes/blob/master/Supervised_Learning/Naive_Bayes_Classifiers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NigNq2doP8TS"
      },
      "source": [
        "## Naive Bayes Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0snLerDsQKfo"
      },
      "source": [
        "Naive Bayes Classifiers are quite similar to linear models, they are even more faster in training. The price paid for this efficiency is that the generalization performance is worse than the linear classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ihrNmWfQ1sp"
      },
      "source": [
        "Naive Bayes models are more efficient because it learn by looking at each features individually and collect simple/class statistics from each\n",
        "feature. In scikit learn there are three classifier, GaussianNB can be applied to continuous data, BernoulliNB assumes binary data and MultinomialNB assumes count data. BernoulliNB and MultinomialNB are mostly used in text data classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiV7MW1KPxot"
      },
      "source": [
        "import numpy as np\n",
        "# The BernoulliNB classifier counts how often every feature of each class is not zero.\n",
        "\n",
        "X = np.array([[0, 1, 0, 1],\n",
        "              [1, 0, 1, 1],\n",
        "              [0, 0, 0, 1],\n",
        "              [1, 0, 1, 0]])\n",
        "y = np.array([0, 1, 0, 1])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuLxbNJNSRg5",
        "outputId": "91dac471-f523-4bae-f27e-e8c9faf65566"
      },
      "source": [
        "counts = {}\n",
        "for label in np.unique(y):\n",
        "  # iterate over each class\n",
        "  # count (sum) entries of 1 per feature\n",
        "  counts[label] = X[y == label].sum(axis=0)\n",
        "print(\"Feature counts:\\n{}\".format(counts))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature counts:\n",
            "{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iASHo9RSm2K"
      },
      "source": [
        "MultinomialNB takes into account the average value of each feature for each class, while GaussianNB stores the average value as well as the standard deviation of each feature for each class. To make a prediction, a data point is compared to the statistics for each of the classes, and the best matching class is predicted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuVu5J5uTBo5"
      },
      "source": [
        "## Strengths, weaknesses, and parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3NL2PCBTG84"
      },
      "source": [
        "MultinomialNB and BernoulliNB have a single parameter, alpha, which controls\n",
        "model complexity. statistics. A large alpha means more smoothing, resulting in less complex models. The algorithmâ€™s performance is relatively robust to the setting of alpha, meaning that setting alpha is not critical for good performance.\n",
        "\n",
        "GaussianNB is mostly used on very high-dimensional data, while the other two variants of naive Bayes are widely used for sparse count data such as text. MultinomialNB usually performs better than BinaryNB "
      ]
    }
  ]
}