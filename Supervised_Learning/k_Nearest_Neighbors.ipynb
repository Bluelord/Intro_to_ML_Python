{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "k-Nearest Neighbors.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM76FicXELfkW3t0ASH9XPQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bluelord/Intro_to_ML_Python_Notes/blob/master/Supervised_Learning/k_Nearest_Neighbors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WD8CbXILyPO"
      },
      "source": [
        "pip install mglearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbjy2ggMOagu"
      },
      "source": [
        "# Importing Libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import mglearn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs-4rW27Oopj"
      },
      "source": [
        "### **k-Nearest Neighbors**\n",
        "---\n",
        "\n",
        "This model consists of sorting the training data to predict the new data point from the closest data point in the traing dataset (nearest neighbours).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GvyZ5WPO6Mr"
      },
      "source": [
        "#### **k-Neighbors classification**\n",
        "\n",
        "In this we consider **k** number of nearest neighbours and uses voting to assign the class for the test data, which means majority class among the k-nearest neighbours will be assigned label to the test data.\n",
        "\n",
        "Following exampl shows the use of three cloest neighbours.\n",
        ".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPPSlQ2tOeNs"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cosmLeRAPFgT"
      },
      "source": [
        "**Analyzing K-Neighbour Classifier**\n",
        "\n",
        "Considering more and more neighbours corresponds to a simpler model. Considering the extreme cases where number of **k** is same as data points in training set, then each test point will have same as the class in training set (underfitting / high bias)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TciK6mHVPEuD"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz1X0PT5POPE"
      },
      "source": [
        "The plot shows that at low **k** value model overfits the training set and has low testing accuracy. As we increase **k** value our model become simpler and the testing accuracy increases. But at very high **k** value our model become too simple and it perform the worse, we will fund the best performance in the middle of the range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EAxK8adPOwz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNQIaimsPVK0"
      },
      "source": [
        "#### **k-neighbours regression**\n",
        "\n",
        "In the with k neighbours, multiple nearest values are found for the test data from the training, the mean of this values are considered to be the prediction for the test data point.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwY-klAiPZAa"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e69fkuNvPckr"
      },
      "source": [
        "**Analyzing K-Nebhours Regressor**\n",
        "\n",
        "Here from the figure we can observe that when k value is low our prediction of test is low and it overfits on training data. As we increase k value to very high it leads to smoother prediction, but these do not fit the training data as well. Thus the k value should we in between so that it fits both training and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu85DAn7Pj4r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huAHbDR4PkLT"
      },
      "source": [
        "#### **Strengths, weaknesses and parameters**\n",
        "\n",
        "* In practic, there are two impotant parameters to KNeighbours Classifiers, one is **k value** and other is how we measure the distanse between data points (By default, Euclidean distance is used and there are many other measure can be used).\n",
        "\n",
        "* One of the strength of KNN is that the it is very easy to understand and also gives reasonable performance without a lot of tuning of the parameters. * When our dataset is very large (either large number of features or sampels numbers) prediction is slow. This model also perform badly when we have sparse datasets (0 most of the time)."
      ]
    }
  ]
}